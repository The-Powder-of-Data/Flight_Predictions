{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#load data\n",
    "df = pd.read_csv('../../data/127000_rand.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    loads sample data extracted from db\n",
    "    \"\"\"\n",
    "    return pd.read_csv('../../data/large_rand.csv') #change this to the path of file\n",
    "\n",
    "######Inital Processing######\n",
    "def remove_rows(df):\n",
    "    \"\"\"\n",
    "    remove outlier rows and certain null values\n",
    "        - arr_delay outliers above 350\n",
    "        - arr_delay where null\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.loc[(df['arr_delay'].notnull()) | (df['arr_delay'] <= 350)]\n",
    "    df = df.loc[df['arr_delay'] <= 350]\n",
    "    #df = df.loc[df['taxi_out'] <= 75]\n",
    "    #df = df.loc[df['taxi_in'] <= 75]\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_datetime(df):\n",
    "    \"\"\"\n",
    "    creates columns for month, year, hour onto the dataframe\n",
    "    \"\"\"\n",
    "    df['month'] = pd.DatetimeIndex(df['fl_date']).month\n",
    "    df['year'] = pd.DatetimeIndex(df['fl_date']).year\n",
    "    df['day'] = pd.DatetimeIndex(df['fl_date']).dayofweek\n",
    "    df['dep_hour'] = df['crs_dep_time'].round(-2)/100\n",
    "    df['arr_hour']= df['crs_arr_time'].round(-2)/100\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_is_late(df):\n",
    "    \"\"\"\n",
    "    creates a column 0/1 to indicate if the flight was late or not\n",
    "    \"\"\"\n",
    "    df['is_late'] = df['arr_delay'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_speed(df):\n",
    "    \"\"\"\n",
    "    create column calculating expected speed of flight. miles/minute\n",
    "    \"\"\"\n",
    "    df['speed'] = (df['distance'] / df['crs_elapsed_time']).round(2)\n",
    "\n",
    "    return df\n",
    "\n",
    " \n",
    "\n",
    "######Table Creations######\n",
    "\n",
    "    ## Airports Table ##\n",
    "def create_table_airports(df=df):\n",
    "    \"\"\"\n",
    "    input flights table and create a unique airports table with stats that will be used for features later\n",
    "    \"\"\"\n",
    "    df_airports = df.copy()\n",
    "    df_airports = df[['origin_airport_id', 'origin', 'origin_city_name']]\n",
    "    df_airports = df_airports.drop_duplicates(subset=['origin_airport_id'])\n",
    "    df_airports.rename(columns={'origin_airport_id': 'airport_id', 'origin':'airport_code'}, inplace=True)\n",
    "    split = df_airports['origin_city_name'].str.split(\",\", n=1, expand=True)\n",
    "    df_airports['city'] = split[0]\n",
    "    df_airports['state'] = split[1]\n",
    "    df_airports.drop('origin_city_name', axis=1, inplace=True)\n",
    "    \n",
    "    return df_airports\n",
    "\n",
    "def create_dep_delay(df_flights, df_airports):\n",
    "    \"\"\"\n",
    "    creates a column showing mean departure delay for each airport\n",
    "    we used mean because previously have 0d out any negatives and mean results in 0 99% of the time\n",
    "    \"\"\"\n",
    "    airports_temp = pd.DataFrame(df_flights.groupby('origin_airport_id').agg('mean')['dep_delay'])\n",
    "    airports_temp['dep_delay'] = airports_temp['dep_delay'].round(2)\n",
    "    airports_temp.reset_index(inplace=True)\n",
    "    airports_temp.rename(columns={'origin_airport_id':'airport_id'}, inplace=True)\n",
    "\n",
    "    df_airports = df_airports.merge(airports_temp, on='airport_id')\n",
    "    df_airports.rename(columns={'dep_delay':'mean_d_delay'}, inplace=True)\n",
    "\n",
    "\n",
    "    return df_airports\n",
    "\n",
    "def create_arr_delay(df_flights, df_airports):\n",
    "    \"\"\"\n",
    "    creates a column showing mean arrival delay for each airport\n",
    "    we used mean because previously have 0d out any negatives and medium results in 0 99% of the time\n",
    "    \"\"\"\n",
    "    airports_temp = pd.DataFrame(df_flights.groupby('dest_airport_id').agg('mean')['arr_delay'])\n",
    "    airports_temp['arr_delay'] = airports_temp['arr_delay'].round(2)\n",
    "    airports_temp.reset_index(inplace=True)\n",
    "    airports_temp.rename(columns={'dest_airport_id':'airport_id'}, inplace=True)\n",
    "\n",
    "    df_airports = df_airports.merge(airports_temp, on='airport_id')\n",
    "    df_airports.rename(columns={'arr_delay':'mean_arr_delay'}, inplace=True)\n",
    "\n",
    "    return df_airports\n",
    "\n",
    "def create_flight_counts(df_flights, df_airports):\n",
    "    \"\"\"\n",
    "    add columns counting amount of flights in and out of the airports\n",
    "    this will then be used to calculate a column about % delayed \n",
    "    \"\"\"\n",
    "    airports_temp = pd.DataFrame(df_flights.groupby('origin_airport_id').count()['flights'])\n",
    "    airports_temp.reset_index(inplace=True)\n",
    "    airports_temp.rename(columns={'origin_airport_id':'airport_id', 'flights':'dep_flight_count'}, inplace=True)\n",
    "    df_airports = df_airports.merge(airports_temp, on='airport_id')\n",
    "    \n",
    "    airports_temp = pd.DataFrame(df_flights.groupby('dest_airport_id').count()['flights'])\n",
    "    airports_temp.reset_index(inplace=True)\n",
    "    airports_temp.rename(columns={'dest_airport_id':'airport_id', 'flights':'arr_flight_count'}, inplace=True)\n",
    "    df_airports = df_airports.merge(airports_temp, on='airport_id')    \n",
    "    \n",
    "    return df_airports\n",
    "\n",
    "def create_delay_counts(df_flights, df_airports):\n",
    "    \"\"\"\n",
    "    create columns counting number of flights lates for depart and arrival airports\n",
    "    \"\"\"\n",
    "    airports_temp = pd.DataFrame(df_flights.groupby(by='origin_airport_id').agg('sum')['is_late'])\n",
    "    airports_temp.reset_index(inplace=True)\n",
    "    airports_temp.rename(columns={'origin_airport_id':'airport_id', 'is_late':'dep_late_count'}, inplace=True)\n",
    "    df_airports = df_airports.merge(airports_temp, on='airport_id')\n",
    "\n",
    "    airports_temp = pd.DataFrame(df_flights.groupby(by='dest_airport_id').agg('sum')['is_late'])\n",
    "    airports_temp.reset_index(inplace=True)\n",
    "    airports_temp.rename(columns={'dest_airport_id':'airport_id', 'is_late':'arr_late_count'}, inplace=True)\n",
    "    df_airports = df_airports.merge(airports_temp, on='airport_id')\n",
    "\n",
    "    return df_airports\n",
    "\n",
    "def create_late_per(df_airports):\n",
    "    \"\"\"\n",
    "    creates columns showing percent late for arrival and departing airports\n",
    "    \"\"\"\n",
    "    df_airports['dep_late_perc'] = (df_airports['dep_late_count'] / df_airports['dep_flight_count']).round(3)\n",
    "    df_airports['arr_late_perc'] = (df_airports['arr_late_count'] / df_airports['arr_flight_count']).round(3)\n",
    "\n",
    "    return df_airports\n",
    "\n",
    "def create_taxi(df_flights, df_airports):\n",
    "    \"\"\"\n",
    "    add columns shows mean taxi time\n",
    "    this will then be used to calculate a column about % delayed \n",
    "    \"\"\"\n",
    "    airports_temp = pd.DataFrame(df_flights.groupby('origin_airport_id').agg('mean')['taxi_out'])\n",
    "    airports_temp.reset_index(inplace=True)\n",
    "    airports_temp.rename(columns={'origin_airport_id':'airport_id', 'taxi_out':'dep_taxi'}, inplace=True)\n",
    "    df_airports = df_airports.merge(airports_temp, on='airport_id')\n",
    "    \n",
    "    airports_temp = pd.DataFrame(df_flights.groupby('dest_airport_id').agg('mean')['taxi_in'])\n",
    "    airports_temp.reset_index(inplace=True)\n",
    "    airports_temp.rename(columns={'dest_airport_id':'airport_id', 'taxi_in':'arr_taxi'}, inplace=True)\n",
    "    df_airports = df_airports.merge(airports_temp, on='airport_id')    \n",
    "    \n",
    "    return df_airports\n",
    "\n",
    "    ## Tail Number Table ##\n",
    "def create_tail_table(df_flights):\n",
    "    \"\"\"\n",
    "    creates tabel with tail number stats from flights table\n",
    "    \"\"\"\n",
    "    df_tail = df_flights[['tail_num', 'fl_date', 'is_late', 'arr_delay', 'arr_time', 'distance', 'carrier_delay', \n",
    "                          'late_aircraft_delay', 'weather_delay']].groupby(by='tail_num').agg({'fl_date':'count', \n",
    "                                                                                               'is_late':'sum', \n",
    "                                                                                               'arr_delay':['median', 'mean', 'std'],\n",
    "                                                                                               'arr_time':['median', 'mean'],\n",
    "                                                                                               'distance':'mean',\n",
    "                                                                                               'carrier_delay':'count',\n",
    "                                                                                               'late_aircraft_delay':'count',\n",
    "                                                                                               'weather_delay':'count'})\n",
    "    df_tail['late_perc'] = (df_tail['is_late']['sum'] / df_tail['fl_date']['count']).round(3)\n",
    "    \n",
    "    #flatterning the multi layered headers for easier use later\n",
    "    flat_cols = list(df_tail.columns.map('_'.join))\n",
    "    df_tail.columns = flat_cols\n",
    "\n",
    "    return df_tail\n",
    "\n",
    "    ## Carrier Table ##\n",
    "def create_carrier_table(df_flights):\n",
    "    \"\"\"\n",
    "    creates tabel with carrier stats from flights table\n",
    "    \"\"\"\n",
    "    df_carrier = df_flights[['mkt_carrier', 'fl_date', 'is_late', \n",
    "                             'dep_delay', 'arr_delay', 'carrier_delay']].groupby(by='mkt_carrier').agg({'fl_date':'count',\n",
    "                                                                                           'is_late':'sum',\n",
    "                                                                                           'dep_delay': 'mean',\n",
    "                                                                                           'arr_delay': 'mean',\n",
    "                                                                                           'carrier_delay': 'mean'})\n",
    "    df_carrier['late_perc'] = (df_carrier['is_late'] / df_carrier['fl_date']).round(3)\n",
    "    df_carrier['carrier_delay'] = df_carrier['carrier_delay'].round(2)\n",
    "    df_carrier['dep_delay'] = df_carrier['dep_delay'].round(2)\n",
    "    df_carrier['arr_delay'] = df_carrier['arr_delay'].round(2)\n",
    "\n",
    "    return df_carrier\n",
    "\n",
    "    ## Hourly Table ##\n",
    "def create_hourly_table(df_flights):\n",
    "    \"\"\"\n",
    "    creates tabel with hourly stats from flights table\n",
    "    \"\"\"\n",
    "    df_hours = df_flights[['dep_hour', 'fl_date', 'is_late', \n",
    "                             'dep_delay', 'arr_delay']].groupby(by='dep_hour').agg({'fl_date':'count',\n",
    "                                                                                           'is_late':'sum',\n",
    "                                                                                           'dep_delay': 'mean',\n",
    "                                                                                           'arr_delay': 'mean'})\n",
    "    df_hours['late_perc'] = (df_hours['is_late'] / df_hours['fl_date']).round(3)\n",
    "    df_hours['dep_delay'] = df_hours['dep_delay'].round(2)\n",
    "    df_hours['arr_delay'] = df_hours['arr_delay'].round(2)\n",
    "\n",
    "    return df_hours\n",
    "\n",
    "    ## Daily Table ##\n",
    "def create_day_table(df_flights):\n",
    "    \"\"\"\n",
    "    creates tabel with day stats from flights table\n",
    "    \"\"\"\n",
    "    df_days = df_flights[['day', 'fl_date', 'is_late', \n",
    "                             'dep_delay', 'arr_delay']].groupby(by='day').agg({'fl_date':'count',\n",
    "                                                                                           'is_late':'sum',\n",
    "                                                                                           'dep_delay': 'mean',\n",
    "                                                                                           'arr_delay': 'mean'})\n",
    "    df_days['late_perc'] = (df_days['is_late'] / df_days['fl_date']).round(3)\n",
    "    df_days['dep_delay'] = df_days['dep_delay'].round(2)\n",
    "    df_days['arr_delay'] = df_days['arr_delay'].round(2)\n",
    "    \n",
    "    #bucket carrier category as orginal\n",
    "\n",
    "    return df_days\n",
    "\n",
    "###### Exporting Created Tables ######\n",
    "def save_tables():\n",
    "    airports.to_csv('../../data/Exported_Tables/stats_airports.csv',  index=False)\n",
    "    tail.to_csv('../../data/Exported_Tables/stats_tail.csv')\n",
    "    carrier.to_csv('../../data/Exported_Tables/stats_carrier.csv')\n",
    "    hourly.to_csv('../../data/Exported_Tables/stats_hourly.csv')\n",
    "    daily.to_csv('../../data/Exported_Tables/stats_daily.csv')\n",
    "\n",
    "\n",
    "###### Loading Created Tables ######\n",
    "def load_f_tables():\n",
    "    airports = pd.read_csv('../../data/Exported_Tables/stats_airports.csv')\n",
    "    tail = pd.read_csv('../../data/Exported_Tables/stats_tail.csv',index_col='tail_num')\n",
    "    carrier = pd.read_csv('../../data/Exported_Tables/stats_carrier.csv', index_col='mkt_carrier')\n",
    "    hourly = pd.read_csv('../../data/Exported_Tables/stats_hourly.csv', index_col='dep_hour')\n",
    "    daily = pd.read_csv('../../data/Exported_Tables/stats_daily.csv', index_col='day')\n",
    "\n",
    "    return airports, tail, carrier, hourly, daily\n",
    "\n",
    "###### Mearging Features Onto Origional Table ######\n",
    "\n",
    "def get_features_airport(df_flights, df_airports):\n",
    "    \"\"\"\n",
    "    merges engineered features from the airport stats table onto the main dataframe as additional columns\n",
    "    \"\"\"\n",
    "    airport_dep = df_airports[['airport_id', 'mean_d_delay', 'dep_late_perc', 'dep_taxi']].rename(columns={'mean_d_delay':'air_mean_d_delay', 'dep_late_perc':'air_dep_late_perc', 'dep_taxi':'air_dep_taxi'})\n",
    "    airport_arr = df_airports[['airport_id', 'mean_arr_delay', 'arr_late_perc', 'arr_taxi']].rename(columns={'mean_arr_delay':'air_mean_arr_delay', 'arr_late_perc':'air_arr_late_perc', 'arr_taxi':'air_arr_taxi'})\n",
    "\n",
    "    #merge each onto the main dataframe and rename\n",
    "\n",
    "    df_temp = df_flights.merge(airport_dep, left_on='origin_airport_id', right_on='airport_id')\n",
    "    df_temp = df_temp.merge(airport_arr, left_on='dest_airport_id', right_on='airport_id')\n",
    "    df_temp.drop(['airport_id_x', 'airport_id_y'], axis = 1, inplace=True)\n",
    "\n",
    "    return df_temp\n",
    "\n",
    "def get_features_hourly(df_flights, df_hourly):\n",
    "    \"\"\"\n",
    "    merges engineered features from the hourly stats table onto the main dataframe as additional columns\n",
    "    \"\"\"\n",
    "    hourly_filter = df_hourly.reset_index()[['dep_hour', 'fl_date', \n",
    "                                             'arr_delay', 'late_perc']].rename(columns={'fl_date':'hour_count', \n",
    "                                                                                        'arr_delay':'hour_arr_delay',\n",
    "                                                                                        'late_perc':'hour_late_perc'})\n",
    "    df_temp = df_flights.merge(hourly_filter, left_on='dep_hour', right_on='dep_hour')\n",
    "    #df_temp.drop(['airport_id_x', 'airport_id_y'], axis = 1, inplace=True)\n",
    "\n",
    "    return df_temp\n",
    "\n",
    "def get_features_tail(df_flights, df_tail):\n",
    "    \"\"\"\n",
    "    merges engineered features from the Tail Num stats table onto the main dataframe as additional columns\n",
    "    \"\"\"\n",
    "    tail_filter = df_tail.reset_index()[['tail_num', 'fl_date_count', 'arr_delay_mean', 'arr_delay_std', 'late_perc_']]\n",
    "    tail_filter.rename(columns={'fl_date_count':'tail_count', 'arr_delay_mean':'tail_arr_delay','arr_delay_std':'tail_arr_delay_std' , 'late_perc_':'tail_late_perc'}, inplace=True)\n",
    "    tail_filter['tail_arr_delay'] = tail_filter['tail_arr_delay'].round(2)\n",
    "    tail_filter['tail_arr_delay_std'] = tail_filter['tail_arr_delay_std'].round(2)\n",
    "    tail_filter.drop('tail_arr_delay_std', axis=1, inplace=True)\n",
    "\n",
    "    df_output = df_flights.merge(tail_filter, left_on='tail_num', right_on='tail_num')\n",
    "\n",
    "    return df_output\n",
    "\n",
    "def get_features_carrier(df_flights, df_carrier):\n",
    "    \"\"\"\n",
    "    merges engineered features from carries\n",
    "    \"\"\"\n",
    "    carrier_filter = df_carrier.reset_index()[['mkt_carrier', 'fl_date', 'arr_delay', \n",
    "                                               'carrier_delay', 'late_perc']].rename(columns={'fl_date':'carrier_count', \n",
    "                                                                                              'arr_delay':'carrier_arr_delay',\n",
    "                                                                                              'carrier_delay':'carrier_carrier_delay',\n",
    "                                                                                              'late_perc':'carrier_late_perc'})\n",
    "    temp = df_flights.merge(carrier_filter, left_on='mkt_carrier', right_on='mkt_carrier')\n",
    "\n",
    "    return temp\n",
    "\n",
    "def get_features_day(df_flights, df_daily):\n",
    "    daily_filter = df_daily.reset_index()[['day', 'fl_date', 'dep_delay', 'arr_delay', \n",
    "                                            'late_perc']].rename(columns={'fl_date':'day_count', \n",
    "                                                                          'dep_delay':'day_dep_delay',\n",
    "                                                                          'arr_delay':'day_arr_delay',\n",
    "                                                                          'late_perc':'day_late_perc'})\n",
    "    \n",
    "    output = df_flights.merge(daily_filter, left_on='day', right_on='day')\n",
    "\n",
    "    return output \n",
    "\n",
    "\n",
    "###### Drop Uneeded Columns ######\n",
    "def drop_it_og(df):\n",
    "    \"\"\"\n",
    "    drop unneeded columns to be used in the model\n",
    "    \"\"\"\n",
    "    output = df.copy().drop(['fl_date', 'mkt_unique_carrier', 'branded_code_share', 'mkt_carrier','mkt_carrier_fl_num', \n",
    "                'op_unique_carrier', 'tail_num', 'op_carrier_fl_num', 'origin_airport_id','origin', 'origin_city_name', 'dest_airport_id',\n",
    "                'dest', 'dest_city_name', 'crs_dep_time', 'dep_time', 'dep_delay', 'taxi_out', 'wheels_off', 'wheels_on', 'taxi_in', \n",
    "                'crs_arr_time', 'arr_time', 'cancelled', 'cancellation_code', 'diverted', 'dup', 'actual_elapsed_time', 'air_time', 'flights',\n",
    "                'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay', 'first_dep_time', 'total_add_gtime', \n",
    "                'longest_add_gtime', 'no_name', 'year'], axis=1)\n",
    "    return output\n",
    "\n",
    "def save_final_features(df):\n",
    "    \"\"\"\n",
    "    export csv final features table to specificed location\n",
    "    \"\"\"\n",
    "    df.to_csv('../../data/Exported_Tables/final_features.csv')\n",
    "    return 'Sucessfully exported final features'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josephluiz/miniconda3/envs/THIRD_LOVE/lib/python3.7/site-packages/ipykernel_launcher.py:3: DtypeWarning: Columns (25) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Basic Proprocessing Complete! ----\n",
      "---- Airport Table Complete! ----\n",
      "---- Tail Table Complete!    ----\n",
      "---- Carrier Table Complete! ----\n",
      "---- hourly Table Complete!  ----\n",
      "---- Daily Table Complete!   ----\n",
      "---- Export Tables Complete! ----\n",
      "---- Loading Complete!       ----\n",
      "---- Merge Complete!         ----\n",
      "---- Final Drop Complete!    ----\n",
      "---- Saving Complete!        ----\n"
     ]
    }
   ],
   "source": [
    "#### LOAD & CLEAN ####\n",
    "df = (\n",
    "            load_data()\n",
    "            .pipe(remove_rows)\n",
    "            .pipe(create_datetime)\n",
    "            .pipe(create_is_late)\n",
    "            .pipe(create_speed)\n",
    "            )\n",
    "print('---- Basic Proprocessing Complete! ----')\n",
    "\n",
    "#### TABLE CREATION ####\n",
    "# create airport table\n",
    "airports = create_table_airports()\n",
    "airports = create_dep_delay(df, airports)\n",
    "airports = create_arr_delay(df, airports)\n",
    "airports = create_flight_counts(df, airports)\n",
    "airports = create_delay_counts(df, airports)\n",
    "airports = create_late_per(airports)\n",
    "airports = create_taxi(df, airports)\n",
    "print('---- Airport Table Complete! ----')\n",
    "\n",
    "# create tail numbers table\n",
    "tail = create_tail_table(df)\n",
    "print('---- Tail Table Complete!    ----')\n",
    "# create carrier table\n",
    "carrier = create_carrier_table(df)\n",
    "print('---- Carrier Table Complete! ----')\n",
    "# create hourly table\n",
    "hourly = create_hourly_table(df)\n",
    "print('---- hourly Table Complete!  ----')\n",
    "# create daily table\n",
    "daily = create_day_table(df)\n",
    "print('---- Daily Table Complete!   ----')\n",
    "# Export all tables for later use #\n",
    "save_tables()\n",
    "print('---- Export Tables Complete! ----')\n",
    "\n",
    "#### Load Created Tables #####\n",
    "airports, tail, carrier, hourly, daily = load_f_tables()\n",
    "print('---- Loading Complete!       ----')\n",
    "\n",
    "#### MERGING TO DF ##### \n",
    "df_merged = df.copy()\n",
    "df_merged = get_features_airport(df_merged, airports)\n",
    "df_merged = get_features_hourly(df_merged, hourly)\n",
    "df_merged = get_features_tail(df_merged, tail)\n",
    "df_merged = get_features_carrier(df_merged, carrier)\n",
    "df_merged = get_features_day(df_merged, daily)\n",
    "print('---- Merge Complete!         ----')\n",
    "\n",
    "#### DROP UNEEEDED ####\n",
    "df_dropped = drop_it_og(df_merged)\n",
    "print('---- Final Drop Complete!    ----')\n",
    "\n",
    "#### DROP UNEEEDED ####\n",
    "save_final_features(df_dropped)\n",
    "print('---- Saving Complete!        ----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fl_date</th>\n",
       "      <th>mkt_unique_carrier</th>\n",
       "      <th>branded_code_share</th>\n",
       "      <th>mkt_carrier</th>\n",
       "      <th>mkt_carrier_fl_num</th>\n",
       "      <th>op_unique_carrier</th>\n",
       "      <th>tail_num</th>\n",
       "      <th>op_carrier_fl_num</th>\n",
       "      <th>origin_airport_id</th>\n",
       "      <th>origin</th>\n",
       "      <th>origin_city_name</th>\n",
       "      <th>dest_airport_id</th>\n",
       "      <th>dest</th>\n",
       "      <th>dest_city_name</th>\n",
       "      <th>crs_dep_time</th>\n",
       "      <th>dep_time</th>\n",
       "      <th>dep_delay</th>\n",
       "      <th>taxi_out</th>\n",
       "      <th>wheels_off</th>\n",
       "      <th>wheels_on</th>\n",
       "      <th>taxi_in</th>\n",
       "      <th>crs_arr_time</th>\n",
       "      <th>arr_time</th>\n",
       "      <th>arr_delay</th>\n",
       "      <th>cancelled</th>\n",
       "      <th>cancellation_code</th>\n",
       "      <th>diverted</th>\n",
       "      <th>dup</th>\n",
       "      <th>crs_elapsed_time</th>\n",
       "      <th>actual_elapsed_time</th>\n",
       "      <th>air_time</th>\n",
       "      <th>flights</th>\n",
       "      <th>distance</th>\n",
       "      <th>carrier_delay</th>\n",
       "      <th>weather_delay</th>\n",
       "      <th>nas_delay</th>\n",
       "      <th>security_delay</th>\n",
       "      <th>late_aircraft_delay</th>\n",
       "      <th>first_dep_time</th>\n",
       "      <th>total_add_gtime</th>\n",
       "      <th>longest_add_gtime</th>\n",
       "      <th>no_name</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>day</th>\n",
       "      <th>dep_hour</th>\n",
       "      <th>arr_hour</th>\n",
       "      <th>is_late</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-19</td>\n",
       "      <td>AS</td>\n",
       "      <td>AS</td>\n",
       "      <td>AS</td>\n",
       "      <td>66</td>\n",
       "      <td>AS</td>\n",
       "      <td>N619AS</td>\n",
       "      <td>66</td>\n",
       "      <td>10299</td>\n",
       "      <td>ANC</td>\n",
       "      <td>Anchorage, AK</td>\n",
       "      <td>10926</td>\n",
       "      <td>CDV</td>\n",
       "      <td>Cordova, AK</td>\n",
       "      <td>1525</td>\n",
       "      <td>1624.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1638.0</td>\n",
       "      <td>1713.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1610</td>\n",
       "      <td>1717.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>45</td>\n",
       "      <td>53.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1532.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2019</td>\n",
       "      <td>6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-19</td>\n",
       "      <td>AS</td>\n",
       "      <td>AS</td>\n",
       "      <td>AS</td>\n",
       "      <td>85</td>\n",
       "      <td>AS</td>\n",
       "      <td>N265AK</td>\n",
       "      <td>85</td>\n",
       "      <td>10397</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>14747</td>\n",
       "      <td>SEA</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>625</td>\n",
       "      <td>622.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>637.0</td>\n",
       "      <td>813.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>845</td>\n",
       "      <td>821.0</td>\n",
       "      <td>-24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>320</td>\n",
       "      <td>299.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2182</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2019</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-19</td>\n",
       "      <td>AS</td>\n",
       "      <td>AS</td>\n",
       "      <td>AS</td>\n",
       "      <td>143</td>\n",
       "      <td>AS</td>\n",
       "      <td>N590AS</td>\n",
       "      <td>143</td>\n",
       "      <td>14057</td>\n",
       "      <td>PDX</td>\n",
       "      <td>Portland, OR</td>\n",
       "      <td>10299</td>\n",
       "      <td>ANC</td>\n",
       "      <td>Anchorage, AK</td>\n",
       "      <td>2040</td>\n",
       "      <td>2032.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>2249.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2320</td>\n",
       "      <td>2253.0</td>\n",
       "      <td>-27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>220</td>\n",
       "      <td>201.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2019</td>\n",
       "      <td>6</td>\n",
       "      <td>20.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-19</td>\n",
       "      <td>AS</td>\n",
       "      <td>AS</td>\n",
       "      <td>AS</td>\n",
       "      <td>317</td>\n",
       "      <td>AS</td>\n",
       "      <td>N442AS</td>\n",
       "      <td>317</td>\n",
       "      <td>11618</td>\n",
       "      <td>EWR</td>\n",
       "      <td>Newark, NJ</td>\n",
       "      <td>14771</td>\n",
       "      <td>SFO</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>1940</td>\n",
       "      <td>2148.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>2347.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2305</td>\n",
       "      <td>231.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>385</td>\n",
       "      <td>463.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2019</td>\n",
       "      <td>6</td>\n",
       "      <td>19.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-19</td>\n",
       "      <td>AS</td>\n",
       "      <td>AS</td>\n",
       "      <td>AS</td>\n",
       "      <td>350</td>\n",
       "      <td>AS</td>\n",
       "      <td>N495AS</td>\n",
       "      <td>350</td>\n",
       "      <td>14747</td>\n",
       "      <td>SEA</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>12266</td>\n",
       "      <td>IAH</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>1200</td>\n",
       "      <td>1201.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1229.0</td>\n",
       "      <td>1827.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1820</td>\n",
       "      <td>1838.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>260</td>\n",
       "      <td>277.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2019</td>\n",
       "      <td>6</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637436</th>\n",
       "      <td>2019-05-19</td>\n",
       "      <td>UA</td>\n",
       "      <td>UA_CODESHARE</td>\n",
       "      <td>UA</td>\n",
       "      <td>3601</td>\n",
       "      <td>YX</td>\n",
       "      <td>N651RW</td>\n",
       "      <td>3601</td>\n",
       "      <td>12266</td>\n",
       "      <td>IAH</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>10693</td>\n",
       "      <td>BNA</td>\n",
       "      <td>Nashville, TN</td>\n",
       "      <td>1620</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1644.0</td>\n",
       "      <td>1811.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1817</td>\n",
       "      <td>1842.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>117</td>\n",
       "      <td>142.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1</td>\n",
       "      <td>657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2019</td>\n",
       "      <td>6</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637437</th>\n",
       "      <td>2019-05-19</td>\n",
       "      <td>UA</td>\n",
       "      <td>UA_CODESHARE</td>\n",
       "      <td>UA</td>\n",
       "      <td>3611</td>\n",
       "      <td>YX</td>\n",
       "      <td>N729YX</td>\n",
       "      <td>3611</td>\n",
       "      <td>13198</td>\n",
       "      <td>MCI</td>\n",
       "      <td>Kansas City, MO</td>\n",
       "      <td>12266</td>\n",
       "      <td>IAH</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>1830</td>\n",
       "      <td>1907.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>2102.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2037</td>\n",
       "      <td>2109.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>127</td>\n",
       "      <td>122.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2019</td>\n",
       "      <td>6</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637438</th>\n",
       "      <td>2019-05-19</td>\n",
       "      <td>UA</td>\n",
       "      <td>UA_CODESHARE</td>\n",
       "      <td>UA</td>\n",
       "      <td>3615</td>\n",
       "      <td>YX</td>\n",
       "      <td>N650RW</td>\n",
       "      <td>3615</td>\n",
       "      <td>13930</td>\n",
       "      <td>ORD</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>13851</td>\n",
       "      <td>OKC</td>\n",
       "      <td>Oklahoma City, OK</td>\n",
       "      <td>1246</td>\n",
       "      <td>1319.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1347.0</td>\n",
       "      <td>1549.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1507</td>\n",
       "      <td>1556.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>141</td>\n",
       "      <td>157.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1</td>\n",
       "      <td>693</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2019</td>\n",
       "      <td>6</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637439</th>\n",
       "      <td>2019-05-19</td>\n",
       "      <td>WN</td>\n",
       "      <td>WN</td>\n",
       "      <td>WN</td>\n",
       "      <td>2980</td>\n",
       "      <td>WN</td>\n",
       "      <td>N296WN</td>\n",
       "      <td>2980</td>\n",
       "      <td>10140</td>\n",
       "      <td>ABQ</td>\n",
       "      <td>Albuquerque, NM</td>\n",
       "      <td>14107</td>\n",
       "      <td>PHX</td>\n",
       "      <td>Phoenix, AZ</td>\n",
       "      <td>1420</td>\n",
       "      <td>1450.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1459.0</td>\n",
       "      <td>1457.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1435</td>\n",
       "      <td>1503.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>75</td>\n",
       "      <td>73.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1</td>\n",
       "      <td>328</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2019</td>\n",
       "      <td>6</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637440</th>\n",
       "      <td>2019-05-19</td>\n",
       "      <td>AS</td>\n",
       "      <td>AS</td>\n",
       "      <td>AS</td>\n",
       "      <td>61</td>\n",
       "      <td>AS</td>\n",
       "      <td>N614AS</td>\n",
       "      <td>61</td>\n",
       "      <td>12523</td>\n",
       "      <td>JNU</td>\n",
       "      <td>Juneau, AK</td>\n",
       "      <td>15991</td>\n",
       "      <td>YAK</td>\n",
       "      <td>Yakutat, AK</td>\n",
       "      <td>1010</td>\n",
       "      <td>959.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>1043.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1105</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>55</td>\n",
       "      <td>47.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "      <td>198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2019</td>\n",
       "      <td>6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>623434 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           fl_date mkt_unique_carrier branded_code_share mkt_carrier  \\\n",
       "0       2019-05-19                 AS                 AS          AS   \n",
       "1       2019-05-19                 AS                 AS          AS   \n",
       "2       2019-05-19                 AS                 AS          AS   \n",
       "3       2019-05-19                 AS                 AS          AS   \n",
       "4       2019-05-19                 AS                 AS          AS   \n",
       "...            ...                ...                ...         ...   \n",
       "637436  2019-05-19                 UA       UA_CODESHARE          UA   \n",
       "637437  2019-05-19                 UA       UA_CODESHARE          UA   \n",
       "637438  2019-05-19                 UA       UA_CODESHARE          UA   \n",
       "637439  2019-05-19                 WN                 WN          WN   \n",
       "637440  2019-05-19                 AS                 AS          AS   \n",
       "\n",
       "        mkt_carrier_fl_num op_unique_carrier tail_num  op_carrier_fl_num  \\\n",
       "0                       66                AS   N619AS                 66   \n",
       "1                       85                AS   N265AK                 85   \n",
       "2                      143                AS   N590AS                143   \n",
       "3                      317                AS   N442AS                317   \n",
       "4                      350                AS   N495AS                350   \n",
       "...                    ...               ...      ...                ...   \n",
       "637436                3601                YX   N651RW               3601   \n",
       "637437                3611                YX   N729YX               3611   \n",
       "637438                3615                YX   N650RW               3615   \n",
       "637439                2980                WN   N296WN               2980   \n",
       "637440                  61                AS   N614AS                 61   \n",
       "\n",
       "        origin_airport_id origin origin_city_name  dest_airport_id dest  \\\n",
       "0                   10299    ANC    Anchorage, AK            10926  CDV   \n",
       "1                   10397    ATL      Atlanta, GA            14747  SEA   \n",
       "2                   14057    PDX     Portland, OR            10299  ANC   \n",
       "3                   11618    EWR       Newark, NJ            14771  SFO   \n",
       "4                   14747    SEA      Seattle, WA            12266  IAH   \n",
       "...                   ...    ...              ...              ...  ...   \n",
       "637436              12266    IAH      Houston, TX            10693  BNA   \n",
       "637437              13198    MCI  Kansas City, MO            12266  IAH   \n",
       "637438              13930    ORD      Chicago, IL            13851  OKC   \n",
       "637439              10140    ABQ  Albuquerque, NM            14107  PHX   \n",
       "637440              12523    JNU       Juneau, AK            15991  YAK   \n",
       "\n",
       "           dest_city_name  crs_dep_time  dep_time  dep_delay  taxi_out  \\\n",
       "0             Cordova, AK          1525    1624.0       59.0      14.0   \n",
       "1             Seattle, WA           625     622.0       -3.0      15.0   \n",
       "2           Anchorage, AK          2040    2032.0       -8.0      16.0   \n",
       "3       San Francisco, CA          1940    2148.0      128.0     119.0   \n",
       "4             Houston, TX          1200    1201.0        1.0      28.0   \n",
       "...                   ...           ...       ...        ...       ...   \n",
       "637436      Nashville, TN          1620    1620.0        0.0      24.0   \n",
       "637437        Houston, TX          1830    1907.0       37.0      15.0   \n",
       "637438  Oklahoma City, OK          1246    1319.0       33.0      28.0   \n",
       "637439        Phoenix, AZ          1420    1450.0       30.0       9.0   \n",
       "637440        Yakutat, AK          1010     959.0      -11.0      12.0   \n",
       "\n",
       "        wheels_off  wheels_on  taxi_in  crs_arr_time  arr_time  arr_delay  \\\n",
       "0           1638.0     1713.0      4.0          1610    1717.0       67.0   \n",
       "1            637.0      813.0      8.0           845     821.0      -24.0   \n",
       "2           2048.0     2249.0      4.0          2320    2253.0      -27.0   \n",
       "3           2347.0      224.0      7.0          2305     231.0      206.0   \n",
       "4           1229.0     1827.0     11.0          1820    1838.0       18.0   \n",
       "...            ...        ...      ...           ...       ...        ...   \n",
       "637436      1644.0     1811.0     31.0          1817    1842.0       25.0   \n",
       "637437      1922.0     2102.0      7.0          2037    2109.0       32.0   \n",
       "637438      1347.0     1549.0      7.0          1507    1556.0       49.0   \n",
       "637439      1459.0     1457.0      6.0          1435    1503.0       28.0   \n",
       "637440      1011.0     1043.0      3.0          1105    1046.0      -19.0   \n",
       "\n",
       "        cancelled cancellation_code  diverted dup  crs_elapsed_time  \\\n",
       "0               0               NaN         0   N                45   \n",
       "1               0               NaN         0   N               320   \n",
       "2               0               NaN         0   N               220   \n",
       "3               0               NaN         0   N               385   \n",
       "4               0               NaN         0   N               260   \n",
       "...           ...               ...       ...  ..               ...   \n",
       "637436          0               NaN         0   N               117   \n",
       "637437          0               NaN         0   N               127   \n",
       "637438          0               NaN         0   N               141   \n",
       "637439          0               NaN         0   N                75   \n",
       "637440          0               NaN         0   N                55   \n",
       "\n",
       "        actual_elapsed_time  air_time  flights  distance  carrier_delay  \\\n",
       "0                      53.0      35.0        1       160           59.0   \n",
       "1                     299.0     276.0        1      2182            NaN   \n",
       "2                     201.0     181.0        1      1542            NaN   \n",
       "3                     463.0     337.0        1      2565            0.0   \n",
       "4                     277.0     238.0        1      1874            0.0   \n",
       "...                     ...       ...      ...       ...            ...   \n",
       "637436                142.0      87.0        1       657            0.0   \n",
       "637437                122.0     100.0        1       643            0.0   \n",
       "637438                157.0     122.0        1       693           33.0   \n",
       "637439                 73.0      58.0        1       328           10.0   \n",
       "637440                 47.0      32.0        1       198            NaN   \n",
       "\n",
       "        weather_delay  nas_delay  security_delay  late_aircraft_delay  \\\n",
       "0                 0.0        8.0             0.0                  0.0   \n",
       "1                 NaN        NaN             NaN                  NaN   \n",
       "2                 NaN        NaN             NaN                  NaN   \n",
       "3                 0.0       78.0             0.0                128.0   \n",
       "4                 0.0       18.0             0.0                  0.0   \n",
       "...               ...        ...             ...                  ...   \n",
       "637436            0.0       25.0             0.0                  0.0   \n",
       "637437            0.0        0.0             0.0                 32.0   \n",
       "637438            0.0       16.0             0.0                  0.0   \n",
       "637439            0.0        0.0             0.0                 18.0   \n",
       "637440            NaN        NaN             NaN                  NaN   \n",
       "\n",
       "        first_dep_time  total_add_gtime  longest_add_gtime  no_name  month  \\\n",
       "0               1532.0             17.0               17.0      NaN      5   \n",
       "1                  NaN              NaN                NaN      NaN      5   \n",
       "2                  NaN              NaN                NaN      NaN      5   \n",
       "3                  NaN              NaN                NaN      NaN      5   \n",
       "4                  NaN              NaN                NaN      NaN      5   \n",
       "...                ...              ...                ...      ...    ...   \n",
       "637436             NaN              NaN                NaN      NaN      5   \n",
       "637437             NaN              NaN                NaN      NaN      5   \n",
       "637438             NaN              NaN                NaN      NaN      5   \n",
       "637439             NaN              NaN                NaN      NaN      5   \n",
       "637440             NaN              NaN                NaN      NaN      5   \n",
       "\n",
       "        year  day  dep_hour  arr_hour  is_late  speed  \n",
       "0       2019    6      15.0      16.0        1   3.56  \n",
       "1       2019    6       6.0       8.0        0   6.82  \n",
       "2       2019    6      20.0      23.0        0   7.01  \n",
       "3       2019    6      19.0      23.0        1   6.66  \n",
       "4       2019    6      12.0      18.0        1   7.21  \n",
       "...      ...  ...       ...       ...      ...    ...  \n",
       "637436  2019    6      16.0      18.0        1   5.62  \n",
       "637437  2019    6      18.0      20.0        1   5.06  \n",
       "637438  2019    6      12.0      15.0        1   4.91  \n",
       "637439  2019    6      14.0      14.0        1   4.37  \n",
       "637440  2019    6      10.0      11.0        0   3.60  \n",
       "\n",
       "[623434 rows x 49 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         AS_ANC_CDV\n",
       "1         AS_ATL_SEA\n",
       "2         AS_PDX_ANC\n",
       "3         AS_EWR_SFO\n",
       "4         AS_SEA_IAH\n",
       "             ...    \n",
       "637436    UA_IAH_BNA\n",
       "637437    UA_MCI_IAH\n",
       "637438    UA_ORD_OKC\n",
       "637439    WN_ABQ_PHX\n",
       "637440    AS_JNU_YAK\n",
       "Length: 623434, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n",
    "df['mkt_carrier'] + \"_\" + df['origin'] + \"_\" + df['dest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "20f4d640321c64eebda1b336220043a55d007188cbae01fe5d177012ab1e8e44"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('THIRD_LOVE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
